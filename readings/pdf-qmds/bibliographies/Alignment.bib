@article{Gabriel2020,
	author = {Iason Gabriel},
	doi = {10.1007/s11023-020-09539-2},
	journal = {Minds and Machines},
	number = {3},
	pages = {411--437},
	publisher = {Springer Verlag},
	title = {Artificial Intelligence, Values, and Alignment},
	volume = {30},
	year = {2020}
}

@article{Dennis2023,
	author = {Matthew Dennis and Elena Ziliotti},
	doi = {10.1111/japp.12627},
	journal = {Journal of Applied Philosophy},
	number = {2},
	pages = {263--279},
	publisher = {Blackwell},
	title = {Living Well Together Online: Digital Wellbeing From a Confucian Perspective},
	volume = {40},
	year = {2023}
}

@inproceedings{Bondi2021,
author = {Bondi, Elizabeth and Xu, Lily and Acosta-Navas, Diana and Killian, Jackson},
year = {2021},
month = {07},
pages = {425-436},
title = {Envisioning Communities: A Participatory Approach Towards AI for Social Good},
doi = {10.1145/3461702.3462612}
}

@article{Han2021,
author = {Han, Shengnan and Kelly, Eugene and Nikou, Shahrokh and Svee, Eric-Oluf},
year = {2021},
month = {07},
pages = {1-13},
title = {Aligning artificial intelligence with human values: reflections from a phenomenological perspective},
volume = {37},
journal = {AI & Society},
doi = {10.1007/s00146-021-01247-4}
}

@inproceedings{Whittlestone2019,
author = {Whittlestone, Jess and Nyrup, Rune and Alexandrova, Anna and Cave, Stephen},
year = {2019},
month = {01},
pages = {195-200},
title = {The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions},
doi = {10.1145/3306618.3314289}
}

@BOOK{Christian2021,
  title     = "The Alignment Problem",
  author    = "Christian, Brian",
  publisher = "WW Norton",
  month     =  oct,
  year      =  2021,
  address   = "New York, NY",
  language  = "en"
}

@misc{Eckersley2019,
  doi = {10.48550/ARXIV.1901.00064},
  url = {https://arxiv.org/abs/1901.00064},
  author = {Eckersley,  Peter},
  keywords = {Artificial Intelligence (cs.AI),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International}
}

@incollection{Thoma2022,
	author = {Johanna Thoma},
	booktitle = {The Cambridge Handbook of Responsible Artificial Intelligence: Interdisciplinary Perspectives},
	editor = {Silja Voeneky and Philipp Kellmeyer and Oliver Mueller and Wolfram Burgard},
	publisher = {Cambridge University Press},
	title = {Risk Imposition by Artificial Agents: The Moral Proxy Problem},
	year = {2022}
}

@article{Sutrop2020,
	author = {Margit Sutrop},
	doi = {10.11590/abhps.2020.2.04},
	journal = {Acta Baltica Historiae Et Philosophiae Scientiarum},
	number = {2},
	pages = {54--72},
	title = {Challenges of Aligning Artificial Intelligence with Human Values},
	volume = {8},
	year = {2020}
}